{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:40.903825Z",
     "start_time": "2023-08-26T08:16:39.935067Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, GridSearchCV\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import GRU, Dense\n",
    "import tensorflow as tf\n",
    "from impala.dbapi import connect\n",
    "from impala.util import as_pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0. 데이터 수집 및 마트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:41.179991Z",
     "start_time": "2023-08-26T08:16:41.179985Z"
    }
   },
   "outputs": [],
   "source": [
    "## 사용변수 ##\n",
    "\n",
    "# 1) 공판장\n",
    "# 경락단가, 상장수수료, 운송비, 하역비, kg당 판매금액 (평균)\n",
    "# 중량 (합계)\n",
    "\n",
    "# 2) 기상청\n",
    "\n",
    "# 3) 재고\n",
    "# nfbpsb.tmp_stpl_trn_inf_03\n",
    "# 차변수량, 매출수량, 재고수량 (합계)\n",
    "# 차변금액, 매출금액, 재고금액 (평균)\n",
    "\n",
    "# 4) 매입\n",
    "# 매입수량 (합계)\n",
    "# 매입금액 (평균)\n",
    "\n",
    "# 5) 소매\n",
    "# 소매가격 (평균)\n",
    "\n",
    "# 6) 도매시장 경락가격\n",
    "# kg당 경락단가 (평균)\n",
    "\n",
    "# 7) 물가지수 (월별)\n",
    "# 품목별 생산자물가지수, 품목성질별 생산자물가지수, 품목별 소비자물가지수, 품목성질별 소비자물가지수\n",
    "\n",
    "# 8) 동향지수 (월별)\n",
    "# 현재생활형편CSI, 생활형편전망CSI, 향후경기전망CSI, 소비지출전망CSI, 외식비 지출전망CSI, 물가수준전망(1년후)CSI\n",
    "\n",
    "# 9) 수출입 (월별)\n",
    "# 수입금액, 수출금액 (평균)\n",
    "# 수입중량, 수출중량 (합계)\n",
    "\n",
    "# 10) 하나로마트\n",
    "# 순매출금액 (합계)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1. Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:41.180753Z",
     "start_time": "2023-08-26T08:16:41.180747Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_df(df): #쿼리\n",
    "    sql = f\"\"\"SELECT * FROM \"\"\"+str(df)\n",
    "\n",
    "    con = connect(host = 'nhbpunloap01.nhbpad.nonghyup.com', port = '21050', \n",
    "                  kerberos_service_name = 'impala', use_ssl=False, auth_mechanism = 'GSSAPI')\n",
    "\n",
    "    impala_cursor = con.cursor() \n",
    "    impala_cursor.execute(sql) #쿼리 실행 \n",
    "    df = as_pandas(impala_cursor) #pandas 데이터 프레임으로 변경\n",
    "    \n",
    "    #byte 타입 변경\n",
    "    for col in df.columns:\n",
    "        if str(df[col][0])[0] == 'b':\n",
    "            df[col] = df[col].str.decode('utf-8')\n",
    "    impala_cursor.close()\n",
    "    con.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#훈련 데이터 전체의 시작일 부터 마지막 일까지의 주차 생성\n",
    "def week_label(df):\n",
    "    \n",
    "    df['week_mark'] = np.nan\n",
    "    df['yyyy'] = df['yyyy'].astype('int')\n",
    "    year_range = int(max(df['yyyy'])) - int(min(df['yyyy']))+1\n",
    "    \n",
    "    for i in range(0,year_range*53):\n",
    "        min_mark = (datetime.strptime(min(df['bas_dt']), '%Y%m%d') + relativedelta(weeks=i)).strftime('%Y%m%d')\n",
    "        max_mark = (datetime.strptime(min(df['bas_dt']), '%Y%m%d') + relativedelta(weeks=i+1)).strftime('%Y%m%d')\n",
    "        \n",
    "        df.loc[(df['bas_dt']>=min_mark)&(df['bas_dt']<max_mark), 'week_mark'] = i\n",
    "    df['week_mark'] = df['week_mark'].astype(int)\n",
    "    df_prp = df.copy()\n",
    "    \n",
    "    return df_prp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기 & Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:41.182400Z",
     "start_time": "2023-08-26T08:16:41.182395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 설명 변수\n",
    "wmc = select_df('nfbpsb.tmp_wmc_trn_inf') #공판장\n",
    "weather = select_df('nfbpsb.tmp_weather_trn_inf') #기상청\n",
    "weather = weather.fillna(0)\n",
    "stpl = select_df('nfbpsb.tmp_stpl_trn_inf') #재고\n",
    "byng = select_df('nfbpsb.tmp_byng_trn_inf') #매입\n",
    "retail = select_df('nfbpsb.tmp_retail_trn_inf') #소매\n",
    "actopr = select_df('nfbpsb.tmp_actopr_trn_inf') #도매시장 경락가격\n",
    "prsix = select_df('nfbpsb.tmp_prsix_trn_inf') #물가지수(월별)\n",
    "tnix = select_df('nfbpsb.tmp_tnix_trn_inf') #동향지수(월별)\n",
    "imxp = select_df('nfbpsb.tmp_imxp_trn_inf') #수출입(월별) (풋고추 없음)\n",
    "nacf_rtl = select_df('nfbpsb.tmp_nacf_rtl_trn_inf') #하나로마트\n",
    "\n",
    "# 타겟 변수\n",
    "sl = select_df('nfbpsb.tmp_sl_trn_inf') #가락시장 가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:41.183194Z",
     "start_time": "2023-08-26T08:16:41.183189Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 일별 테이블 한번에 붙이기\n",
    "datasets = [wmc, stpl, byng, retail, actopr, nacf_rtl]\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on = ['frpd_latcnm', 'bas_dt', 'bas_week'],\n",
    "                                         how = 'outer'), datasets)\n",
    "# 기상 테이블 join\n",
    "df = pd.merge(df, weather, on = ['bas_dt', 'bas_week'], how = 'outer')\n",
    "\n",
    "# 월별 테이블 붙이기 (월별테이블은 월 말에 적재되는 이유로 전월 데이터만 사용 예정)\n",
    "df['bas_dt'] = pd.to_datetime(df['bas_dt'])\n",
    "df['bas_ym'] = df['bas_dt'].dt.to_period('M')\n",
    "df['bas_ym'] = df['bas_ym'].dt.strftime('%Y%m')\n",
    "df['bas_dt'] = df['bas_dt'].dt.strftime('%Y%m%d')\n",
    "datasets = [df, prsix, imxp]\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on = ['frpd_latcnm', 'bas_ym'],\n",
    "                                         how = 'outer'), datasets)\n",
    "df = pd.merge(df, tnix, on = 'bas_ym', how = 'outer')\n",
    "\n",
    "# 가락시장 가격 테이블에 left join\n",
    "df = pd.merge(garak_price_avg, df, on = ['frpd_latcnm', 'bas_dt'], how = 'left')\n",
    "\n",
    "# 요일, 년도, 월, 일, 주차 생성\n",
    "df['bas_dt'] = pd.to_datetime(df['bas_dt'])\n",
    "df['weekday'] = df['bas_dt'].dt.day_name()\n",
    "df['yyyy'] = df['bas_dt'].dt.year\n",
    "df['mm'] = df['bas_dt'].dt.month\n",
    "df['dd'] = df['bas_dt'].dt.day\n",
    "df['week'] = df['bas_week'].str[4:].astype(int)\n",
    "df['bas_dt'] = df['bas_dt'].dt.strftime('%Y%m%d')\n",
    "\n",
    "drop_idx = df.loc[df['weekday']=='Sunday'].index #일요일 삭제\n",
    "df.drop(drop_idx, inplace=True)\n",
    "\n",
    "# 요일 컬럼 더미변수화\n",
    "weekday_dummies = pd.get_dummies(df['weekday'])\n",
    "df = pd.concat([df, weekday_dummies], axis = 1)\n",
    "\n",
    "# 훈련 데이터 전체의 시작일 부터 마지막 일까지의 주차 생성\n",
    "df = week_label(df)\n",
    "\n",
    "df = df.sort_values(['frpd_latcnm', 'bas_dt']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:41.184238Z",
     "start_time": "2023-08-26T08:16:41.184234Z"
    }
   },
   "outputs": [],
   "source": [
    "# 주차별 표준편차 계산\n",
    "def price_std_vy_frpd(datasets, frpd_latc_c):\n",
    "    df = datasets.loc[datasets['frpd_latc_c'] == frpd_latc_c].drop(['frpd_latc_c'], axis = 1)\n",
    "    std = pd.DataFrame(df.groupby('week')['gk_price'].std(ddof = 0)).reset_index()\n",
    "    return std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품목별 DataSet 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:45.005151Z",
     "start_time": "2023-08-26T08:16:44.995992Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_preprocess(datasets, frpd_latcnm):\n",
    "    \n",
    "    df = datasets.loc[datasets['frpd_latcnm'] == frpd_latcnm].drop(['frpd_latcnm'], axis = 1)\n",
    "    \n",
    "    # 1) 가격변수/물량변수/기상변수(강수계속시간, 일강수량, 안개계속시간) \n",
    "    # 주평균 -> 월평균\n",
    "    # 날짜 제외한 모든 변수 데이터 타입 변경\n",
    "    cols_date = ['frpd_latcnm', 'bas_dt', 'bas_ym', 'bas_week', 'yyyy', 'mm', 'dd', 'week', 'weekday', 'week_mark',\n",
    "                 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "    cols_all = [col for col in list(df.columns) if col not in cols_date]\n",
    "    for col in cols_all:\n",
    "        df[col] = df[col].astype(float)\n",
    "    col_am_mqt = ['gk_price', 'gk_mqt', 'acto_upr', 'lstg_fee', 'trpcs', 'stvcs', 'wt', \n",
    "                  'sel_am', 'db_wt', 'db_am', 'sl_wt', 'slam', 'stpl_wt', 'stpl_am',\n",
    "                  'byng_mqt', 'byam', 'price', 'tot_qty', 'whsl_acto_trqt', 'nslam']\n",
    "    col_weather = ['sumrndur_south', 'sumrndur_mid', 'sumrn_south', 'sumrn_mid', 'sumfogdur_south', 'sumfogdur_mid']\n",
    "    column_to_fill = col_am_mqt + col_weather\n",
    "\n",
    "    # 주 평균으로 채우기\n",
    "    weekly_avg = df.groupby(['bas_week'])[column_to_fill].transform('mean')\n",
    "    df[column_to_fill].fillna(weekly_avg, inplace = True)\n",
    "\n",
    "    # 월 평균으로 채우기 (주 평균으로 null값 채워지지 않을경우 대비)\n",
    "    monthly_avg = df.groupby(['bas_ym'])[column_to_fill].transform('mean')\n",
    "    df[column_to_fill].fillna(monthly_avg, inplace = True)\n",
    "\n",
    "    # 2) 나머지 변수 -> ffill/bfill\n",
    "    df = df.fillna(method = 'ffill').fillna(method = 'bfill')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:46.492061Z",
     "start_time": "2023-08-26T08:16:46.393079Z"
    }
   },
   "outputs": [],
   "source": [
    "df_potato = df_preprocess(df, '감자')\n",
    "df_leak = df_preprocess(df, '대파')\n",
    "df_radish = df_preprocess(df, '무')\n",
    "df_cabbage = df_preprocess(df, '배추')\n",
    "df_apple = df_preprocess(df, '사과')\n",
    "df_pepper = df_preprocess(df, '풋고추')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. 파생변수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시차변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:16:56.614670Z",
     "start_time": "2023-08-26T08:16:56.600913Z"
    }
   },
   "outputs": [],
   "source": [
    "# 시차변수 생성\n",
    "def lag_variable(df, y_col, col_type):\n",
    "    \n",
    "    if col_type == 'target':\n",
    "        # 1일 ~ 24일 후 값\n",
    "        for day in range(1, 25):\n",
    "            df[y_col+'_'+str(day)+'day_after'] = df[y_col].shift(-day)\n",
    "    \n",
    "    if col_type == 'day':\n",
    "        # 1일 전 값\n",
    "        day_ls = [1]\n",
    "        for day in day_ls:\n",
    "            df[y_col+'_'+str(day)+'day_ago'] = df[y_col].shift(day)\n",
    "\n",
    "        # 1주, 4주 전 값\n",
    "        week_ls = [1,4]\n",
    "        for week in week_ls:\n",
    "            df[y_col+'_'+str(week)+'wk_ago'] = df.apply(lambda row: df[(df['week_mark'] == row['week_mark'] - week) \n",
    "                                                                  & (df['weekday'] == row['weekday'])][y_col].mean(), axis = 1)\n",
    "\n",
    "        # 지난 3/5/7/15일간 평균\n",
    "        day_ls = [3,5,7,15]\n",
    "        for day in day_ls:\n",
    "            df[y_col+'_'+str(day)+'day_avg'] = df[y_col].rolling(window=day).mean()\n",
    "\n",
    "        # 1년 전 동일 월 평균\n",
    "        year_ls = [1]\n",
    "        for year in year_ls:\n",
    "            df[y_col+'_'+str(year)+'ym_avg'] = df.apply(lambda row: df[(df['yyyy'] == row['yyyy'] - year) \n",
    "                                                                  & (df['mm'] == row['mm'])][y_col].mean(), axis = 1)\n",
    "\n",
    "        # 1년 전 동일 주차 평균\n",
    "        year_ls = [1]\n",
    "        for year in year_ls:\n",
    "            df[y_col+'_'+str(year)+'yw_avg'] = df.apply(lambda row: df[(df['week_mark'] == row['week_mark'] - 52) \n",
    "                                                                      ][y_col].mean(), axis = 1)\n",
    "            \n",
    "    if col_type == 'month':\n",
    "        # 1월 전\n",
    "        month_ls = [1]\n",
    "        for month in month_ls:\n",
    "            df[y_col+'_'+str(month)+'month_ago'] = df.apply(lambda row: df[(df['bas_ym'] == (pd.to_datetime(row['bas_dt']) - relativedelta(months = month)).strftime('%Y%m'))\n",
    "                                                                          ][y_col].mean(), axis = 1)\n",
    "        \n",
    "        # 1년 전 동일 월 평균\n",
    "        year_ls = [1]\n",
    "        for year in year_ls:\n",
    "            df[y_col+'_'+str(year)+'ym_avg'] = df.apply(lambda row: df[(df['yyyy'] == row['yyyy'] - year) \n",
    "                                                                  & (df['mm'] == row['mm'])][y_col].mean(), axis = 1)\n",
    "    \n",
    "    df.reset_index(drop = True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:27:18.682880Z",
     "start_time": "2023-08-26T08:16:57.599426Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_potato 실행시간: 104.91838884353638초\n",
      "df_leak 실행시간: 101.28769207000732초\n",
      "df_radish 실행시간: 102.7426393032074초\n",
      "df_cabbage 실행시간: 105.28370118141174초\n",
      "df_apple 실행시간: 102.84782314300537초\n",
      "df_pepper 실행시간: 103.99173378944397초\n"
     ]
    }
   ],
   "source": [
    "df_all = {'df_potato':df_potato, 'df_leak':df_leak, 'df_radish':df_radish,\n",
    "          'df_cabbage':df_cabbage, 'df_apple':df_apple, 'df_pepper':df_pepper}\n",
    "df_all_preprocessed = []\n",
    "for key, value in df_all.items():\n",
    "    start_time = time.time()\n",
    "    # 시차변수 생성을 위한 dataframe\n",
    "    df_lag = value.copy()\n",
    "\n",
    "    # 시차변수 생성 (일/월)\n",
    "    # 일별컬럼\n",
    "    for col in cols_day:\n",
    "        df_lag = lag_variable(df_lag, col, 'day')\n",
    "    \n",
    "    # 월별컬럼\n",
    "    for col in cols_month:\n",
    "        df_lag = lag_variable(df_lag, col, 'month')\n",
    "        \n",
    "    # 타겟 변수\n",
    "    df_lag = lag_variable(df_lag, 'gk_price', 'target')\n",
    "    \n",
    "    # 전년동월/주차 시차변수를 생성할 수 없는 값들을 drop\n",
    "    # 2016년~ 데이터를 생성한 뒤 2017년 데이터부터 사용하는 방법\n",
    "    df_lag = df_lag.loc[df_lag['yyyy'] != 2016].reset_index(drop = True)\n",
    "    \n",
    "    # 당월변수 삭제 (월별테이블은 월 말에 적재되는 이유로 전월 데이터만 사용 예정)\n",
    "    df_lag.drop(cols_month, axis = 1, inplace = True)\n",
    "    \n",
    "    # null값 전처리\n",
    "    df_lag = df_lag.fillna(method = 'ffill').fillna(method = 'bfill')\n",
    "\n",
    "    df_all_preprocessed.append(df_lag)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"{key} 실행시간: {execution_time}초\")\n",
    "    \n",
    "df_potato = df_all_preprocessed[0]\n",
    "df_leak = df_all_preprocessed[1]\n",
    "df_radish = df_all_preprocessed[2]\n",
    "df_cabbage = df_all_preprocessed[3]\n",
    "df_apple = df_all_preprocessed[4]\n",
    "df_pepper = df_all_preprocessed[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 중요도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T08:30:12.717762Z",
     "start_time": "2023-08-26T08:30:12.713202Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def analyze_regression(X, y):\n",
    "    # 회귀분석 수행\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    p_values = model.pvalues[1:]\n",
    "    coef_abs = np.abs(model.params[1:])\n",
    "    return p_values, coef_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = {'price_potato':df_potato, 'price_leak':df_leak, 'price_radish':df_radish,\n",
    "          'price_cabbage':df_cabbage, 'price_apple':df_apple, 'price_pepper':df_pepper}\n",
    "\n",
    "importance_xgb_all = []\n",
    "importance_reg_all = []\n",
    "\n",
    "for key, value in df_all.items():\n",
    "    df_tmp = value.copy()\n",
    "    \n",
    "    df_tmp = df_tmp.drop(cols_pk, axis = 1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    X = df_tmp.drop(cols_target, axis = 1)\n",
    "    Y = df_tmp['gk_price_1day_after']\n",
    "\n",
    "    # XGBoost Regressor\n",
    "    model = XGBRegressor(objective = 'reg:squarederror', n_estimators=100, random_state=42)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    importances_xgb = model.feature_importances_\n",
    "\n",
    "    importance_xgb_df = pd.DataFrame({'Feature':X.columns, 'Importance':importances_xgb})\n",
    "    importance_xgb_df = importance_xgb_df.sort_values(by = 'Importance', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    importance_xgb_all.append(importance_xgb_df)\n",
    "    \n",
    "    # Linear Regression\n",
    "    p_values, coef_abs = analyze_regression(X, Y)\n",
    "    \n",
    "    variable_importance = dict(zip(X.columns, coef_abs))\n",
    "\n",
    "    importance_reg_df = pd.DataFrame({\n",
    "        'Columns': list(variable_importance.keys()),\n",
    "        'p_value': p_values,\n",
    "        'Importance': coef_abs\n",
    "    })\n",
    "    importance_reg_df = importance_reg_df[importance_reg_df['p_value'] < 0.05] # 유의한 변수만\n",
    "    importance_reg_df = importance_reg_df.sort_values(by = 'Importance', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    importance_reg_all.append(importance_reg_df)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4. Model 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_same_week(df, column_name):\n",
    "    unique_weeks = df[column_name].unique()\n",
    "    sampled_rows = []\n",
    "    \n",
    "    for week in unique_weeks:\n",
    "        week_rows = df[df[column_name] == week]\n",
    "        sampled_row = week_rows.sample(n = 1, random_state = 42)\n",
    "        sampled_rows.append(sampled_row)\n",
    "    \n",
    "    result_df = pd.concat(sampled_rows)\n",
    "    return result_df\n",
    "\n",
    "# dataset for GRU\n",
    "def make_dataset(data, label, window_size):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true-y_pred)/y_true))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = {'price_potato':df_potato, 'price_leak':df_leak, 'price_radish':df_radish,\n",
    "          'price_cabbage':df_cabbage, 'price_apple':df_apple, 'price_pepper':df_pepper}\n",
    "# 성능지표 저장\n",
    "eval_df_all = []\n",
    "# 변수 중요도 저장\n",
    "importance_df_all = []\n",
    "# 결과 저장\n",
    "results_all = []\n",
    "\n",
    "for key, value in df_all.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if key == 'price_potato': frpd_latcnm = '감자'\n",
    "    if key == 'price_leak': frpd_latcnm = '대파'\n",
    "    if key == 'price_radish': frpd_latcnm = '무'\n",
    "    if key == 'price_cabbage': frpd_latcnm = '배추'\n",
    "    if key == 'price_apple': frpd_latcnm = '사과'\n",
    "    if key == 'price_pepper': frpd_latcnm = '풋고추'\n",
    "    \n",
    "    df_tmp = value.copy()\n",
    "    \n",
    "    ##############################################################################\n",
    "    ## Variable Selection\n",
    "    df_tmp = df_tmp.drop(cols_pk, axis = 1)\n",
    "    X = df_tmp.drop(cols_target, axis = 1)\n",
    "    Y = df_tmp['gk_price_1day_after']\n",
    "\n",
    "    model = XGBRegressor(objective = 'reg:squarederror', n_estimators=100, random_state=42)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    importances_xgb = model.feature_importances_\n",
    "\n",
    "    importance_xgb_df = pd.DataFrame({'Feature':X.columns, 'Importance':importances_xgb})\n",
    "    importance_xgb_df = importance_xgb_df.sort_values(by = 'Importance', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    ##############################################################################\n",
    "    df_tmp = value.copy()\n",
    "    \n",
    "    # TOP100 변수 선택\n",
    "    cols_selected = list(importance_xgb_df['Feature'].head(100)) #변수 개수 지정\n",
    "    cols_model = cols_pk + cols_target + cols_selected\n",
    "    df_tmp = df_tmp[cols_model]\n",
    "    \n",
    "    # 주별로 하나의 값만 random sampling\n",
    "    test_sampled = random_sample_same_week(test_tmp, 'bas_week')\n",
    "    # 나머지 row들은 학습에 사용\n",
    "    remaining_indices = list(set(test_tmp.index) - set(test_sampled.index))\n",
    "    remaining_test = test_tmp.loc[remaining_indices].reset_index(drop = True)\n",
    "    train_tmp = pd.concat([train_tmp, remaining_test], ignore_index = True)\n",
    "\n",
    "    test_tmp = test_sampled.copy()\n",
    "\n",
    "    train_data = train_tmp.drop(cols_pk, axis = 1)\n",
    "    test_data = test_tmp.drop(cols_pk, axis = 1)\n",
    "    \n",
    "    ## cols_target\n",
    "    # 훈련 데이터셋 생성\n",
    "    X_train = train_data.drop(cols_target, axis = 1)\n",
    "    y_train = train_data[cols_target]\n",
    "\n",
    "    # 테스트 데이터셋 생성\n",
    "    X_test = test_data.drop(cols_target, axis = 1)\n",
    "    y_test = test_data[cols_target]\n",
    "        \n",
    "    ##############################################################################\n",
    "    ### XGBoost\n",
    "    print('XGBoost Model Training...')\n",
    "\n",
    "    # TimeSeriesSplit 객체 생성\n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "    # hyperparameter 그리드 정의\n",
    "    param_grid = {\n",
    "        'estimator__n_estimators': [100, 200, 300],\n",
    "        'estimator__max_depth': [3, 4, 5],\n",
    "        'estimator__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'estimator__colsample_bytree':[0.3, 0.7, 1.0]\n",
    "        }\n",
    "\n",
    "    # Grid Search 객체 생성\n",
    "    grid_search = GridSearchCV(\n",
    "        MultiOutputRegressor(XGBRegressor(objective = 'reg:squarederror')), \n",
    "        param_grid = param_grid, \n",
    "        scoring = 'neg_mean_absolute_error', # 평가지표\n",
    "        cv = tscv # TimeSeriesSplit\n",
    "        )\n",
    "\n",
    "    # 최적의 하이퍼파라미터\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    ##############################################################################\n",
    "    ### GRU\n",
    "    print('GRU Model Training...')\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxscaler()\n",
    "    x_train_GRU = pd.DataFrame(scaler.fit_transform(x_train_GRU))\n",
    "    y_train_GRU = pd. DataFrame(scaler.fit_transform(y_train_GRU))\n",
    "    x_test_GRU = pd.DataFrame(scaler.fit_transform(x_test_GRU))\n",
    "    y_test_GRU = pd. DataFrame(scaler.fit_transform(y_test_GRU))\n",
    "\n",
    "    look_back = 6 # 입력 시퀀스 길이\n",
    "    Target_horizon = 24 # 모델의 출력 차원\n",
    "\n",
    "    x_train_window, y_train_window = make_dataset(x_train_GRU, y_train_GRU, look_back)\n",
    "    x_test_wIndow, y_test_window = make_dataset(x_test_GRU, y_test_GRU, look_back)\n",
    "\n",
    "    def create_model(look_back, target_horizon):\n",
    "        model = Sequential()\n",
    "        model.add(GRU(units = 128, activation = 'relu', input_shape = (look_back, 100, return_sequences = False)))\n",
    "        model.add(Dense(target_horizon))\n",
    "        model.compile(optimizer = 'adam', loss = 'mse')\n",
    "        return model\n",
    "    \n",
    "    # 모델 래퍼 생성\n",
    "    model = KerasRegressor(build_fn = create_model, look_back = look_back, target_horizon = target_horizon, verbose = 2)\n",
    "\n",
    "    # hyperparameter 그리드 정의\n",
    "    param_grid = {'epochs': [10,20,30],\n",
    "                'batch_size': [16,32,64]\n",
    "                }\n",
    "\n",
    "    # 시계열 교차 검증\n",
    "    tscv = TimeSeriesSplit(n_splits = 6)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator = model, \n",
    "        param_grid = param_grid,\n",
    "        cv = tscv # TimeSeriesSplit\n",
    "        )\n",
    "\n",
    "    grid_result = grid_search.fit(x_train_window, y_train_window, callbacks = [early_stop])\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # 예측\n",
    "    test_preds = best_model.predict(X_test)\n",
    "\n",
    "    ##############################################################################\n",
    "    # Test 성능\n",
    "    mse = mean_squared_error(y_test, test_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, test_preds)\n",
    "    mape = mean_absolute_percentage_error(y_test, test_preds)\n",
    "    r2 = r2_score(y_test, test_preds)\n",
    "    \n",
    "    print(f\"{key} Test 성능\")\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
    "    print(\"R-Squared:\", r2)\n",
    "    \n",
    "    ## Test 성능 저장\n",
    "    # 운영 시 -> '기준일자', '기준주차' 컬럼 필요\n",
    "    eval_df = pd.DataFrame({'Frpd_latcnm': [frpd_latcnm],\n",
    "                            'Model_Name':['Price Prediction - XGBRegressor'],\n",
    "                            'Model_Object' :[best_model],\n",
    "                            'MSE':[mse],\n",
    "                            'RMSE':[rmse],\n",
    "                            'MAE':[mae],\n",
    "                            'MAPE':[mape]})\n",
    "    \n",
    "    eval_df_all.append(eval_df)\n",
    "    \n",
    "    ##############################################################################\n",
    "    # Train 성능\n",
    "    train_preds = best_model.predict(X_train)\n",
    "\n",
    "    mse = mean_squared_error(y_train, train_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_train, train_preds)\n",
    "    mape = mean_absolute_percentage_error(y_train, train_preds)\n",
    "    r2 = r2_score(y_train, train_preds)\n",
    "\n",
    "    print(f\"{key} Train 성능\")\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
    "    print(\"R-Squared:\", r2)\n",
    "    \n",
    "    ##############################################################################\n",
    "    ## 변수 중요도 저장\n",
    "    importance_data  = []\n",
    "    feature_names = list(X_train.columns)\n",
    "    for i, model in enumerate(best_model.estimators_):\n",
    "        importance = model.feature_importances_\n",
    "        ranked_features = np.argsort(importance)[::-1] #중요도 내림차순 정렬 후 인덱스 반환\n",
    "        for rank, j in enumerate(ranked_features):\n",
    "            importance_data.append([frpd_latcnm, 'Price Prediction - XGBRegressor',\n",
    "                                    f\"Model {i+1}\", rank+1, feature_names[j], importance[j]])\n",
    "\n",
    "    columns = ['Frpd_latcnm', 'Model_Name', 'Model_Num', 'Rank', 'Feature', 'Importance']\n",
    "    importance_df = pd.DataFrame(importance_data, columns = columns)\n",
    "    importance_df_all.append(importance_df)\n",
    "    \n",
    "    ##############################################################################\n",
    "    ## 시각화를 위한 실제값/예측값 저장    \n",
    "    # 실제값\n",
    "    y_averaged_columns = [y_test.iloc[:, i:i+6].mean(axis=1) for i in range(0, len(y_test.columns), 6)]\n",
    "    y_test_week = pd.concat(y_averaged_columns, axis=1).reset_index(drop = True)\n",
    "    new_column = {0: '실제값 - 1주차', 1: '실제값 - 2주차', 2: '실제값 - 3주차', 3: '실제값 - 4주차'}\n",
    "    y_test_week.rename(columns = new_column, inplace = True)\n",
    "    \n",
    "    # 예측값\n",
    "    test_preds = pd.DataFrame(test_preds)\n",
    "    pred_averaged_columns = [test_preds.iloc[:, i:i+6].mean(axis=1) for i in range(0, len(test_preds.columns), 6)]\n",
    "    pred_week = pd.concat(pred_averaged_columns, axis=1).reset_index(drop = True)\n",
    "    new_column = {0: '예측값 - 1주차', 1: '예측값 - 2주차', 2: '예측값 - 3주차', 3: '예측값 - 4주차'}\n",
    "    pred_week.rename(columns = new_column, inplace = True)\n",
    "    \n",
    "    date_df = test_tmp[['bas_dt', 'bas_ym', 'bas_week']].reset_index(drop = True)\n",
    "    results = pd.concat([date_df, y_test_week.iloc[:,0], pred_week.iloc[:,0], y_test_week.iloc[:,1], pred_week.iloc[:,1], \n",
    "                         y_test_week.iloc[:,2], pred_week.iloc[:,2], y_test_week.iloc[:,3], pred_week.iloc[:,3]], axis = 1)\n",
    "    \n",
    "    results_all.append(results)\n",
    "    \n",
    "    ##############################################################################\n",
    "    ## 모델 저장\n",
    "    model_name = f\"{key}.pkl\"\n",
    "    with open(model_name, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"{key} model saved.\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"{key} 실행시간: {execution_time}초\")\n",
    "    print('==='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = {'Potato':results_potato, 'Leak':results_leak, 'Radish':results_radish,\n",
    "          'Cabbage':results_cabbage, 'Apple':results_apple, 'Pepper':results_pepper}\n",
    "\n",
    "for key, value in result.items():\n",
    "    for i in range(1, 5):\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        plt.plot(value[f\"실제값 - {i}주차\"], label = 'Original Data', color = 'blue')\n",
    "        plt.plot(value[f\"예측값 - {i}주차\"], label = 'Predicted Data', color = 'red')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.xticks(range(0, 52), value['bas_dt'])\n",
    "        plt.title(f\"{key} Week {i} Prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
